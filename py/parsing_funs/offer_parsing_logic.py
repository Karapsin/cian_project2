import re
import pandas as pd
import shutil

from random import shuffle, choice
from py.constants.json_keys_lists import keys_meta_list
from py.utils.utils import (
    time_print,
    save_file,
    random_sleep,
    get_url_based_name,
    load_offer_json,
    add_json_values
)

async def parse_offer_page(scraper, 
                           url, 
                           search_photos_url,
                           visited_before
          ):

    # creating a directory in which info about current url will be saved
    path = f"data_load_temp/{get_url_based_name(url)}" 
    dest_dir = "data_load" 

    ########################################################################################################
    # get json which will be parsed further
    offer_json = await load_offer_json(scraper, url, 'offer_page', path)
    single_ad_df = pd.DataFrame()

    # just a shorthand notation to improve readability further 
    ad_data = offer_json['offerData']['offer']

    ########################################################################################################
    # values which can be parsed easily
    
    # loaded json have a complicated nested structure
    # this section adds to our one row df a lot of values
    # from a various internal jsons (the ones which are in the json_list object) 

    # for every such internal json we have a lot of keys to check,
    # to improve readability all of them are combined in the keys_meta_list obejcts
    
    #from json_keys_lists import keys_meta_list
    json_list = [ad_data['newbuilding'], 
                 ad_data['building'], 
                 offer_json['offerData'].get("bti", {}).get("houseData", {}), 
                 ad_data
                ]

    # the next step is to apply add_json_values function for every json - key list pair,
    # that function simply creates a column in the df 
    # which name is the same as the current key
    # and the value is extracted from the json using the mentioned key
    # (values is set to None if there is no such key in the current json)

    for json, keys_list in zip(json_list, keys_meta_list):
        single_ad_df = add_json_values(single_ad_df, json, keys_list)

    single_ad_df['object_guid'] = offer_json['offerData']['offer']['objectGuid']
    
    #######################################################################################################
    # some values which needs to be parsed separately

    # if we failed to get a buildYear at the previous step
    # we add it here
    if single_ad_df['buildYear'][0] is None and "bti" in offer_json['offerData']:
        house_data = offer_json['offerData']['bti']['houseData'] 
        single_ad_df['buildYear'] = house_data['yearRelease'] if 'yearRelease' in house_data else None

    # some inner jsons have the same structure, and hence
    # the parsing procedure is always the same
    # 2 following functions are just shortcuts for that procedure
    def search_for_label(input_json, needed_label):
        result = [x['value'] 
                  for x 
                  in input_json
                  if (1==1
                       and 'label' in x 
                       and 'value' in x 
                       and x['label'] == needed_label
                    )
                 ]
        return result

    def none_if_empty(input_list):
        return input_list[0] if len(input_list) > 0 else None
    
    # wc = water closet
    wc_type = search_for_label(offer_json['offerData']['features'][0]['features'], 'Санузел')
    single_ad_df['wc_type'] = none_if_empty(wc_type)

    # the same idea as with the wc, but for the elevator
    elevator_type = search_for_label(offer_json['offerData']['features'][0]['features'] , 'Количество лифтов')
    single_ad_df['elevator_descr'] = none_if_empty(elevator_type)

    # sale terms (свободная, альтернативная продажа)
    sale_terms = search_for_label(offer_json['offerData']['sidebar'], 'Условия сделки')
    single_ad_df['sale_terms'] = none_if_empty(sale_terms)

    # ad views
    if 'stats' in offer_json['offerData'].keys():
        single_ad_df['today_views'] = offer_json['offerData']['stats']['daily']
        single_ad_df['total_views'] = offer_json['offerData']['stats']['total']
    else:
        single_ad_df['today_views'], single_ad_df['total_views'] = None, None

    # price history 
    price_history = [(x['changeTime'], x['priceData']['price']) 
                    for x 
                    in offer_json['offerData']['priceChanges']
                    ]

    single_ad_df['price_history'] = str(price_history)

    # seo stuff generated by cian to promote the ad
    # (text displayed on the cian or vk ads)
    seo_data = offer_json['offerData']['seoData']
    single_ad_df['seo_media_title_full'] = seo_data['socialNetworksTitle']['full']
    single_ad_df['seo_media_title_short'] = seo_data['socialNetworksTitle']['short']
    single_ad_df['seo_main_title'] = seo_data['mainTitle']
    single_ad_df['seo_descr'] = seo_data['description']
    single_ad_df['sidebar_info'] = str(offer_json['offerData']['sidebar'])
    single_ad_df['current_desc'] = ad_data['description']
    
    is_closed = ad_data['description'] == "Объявление снято с публикации, поищите ещё что-нибудь"
    single_ad_df['ad_is_closed'] = is_closed

    # no need to load photos if it was visited before
    if visited_before:
        shutil.move(path, dest_dir)
        return single_ad_df

    #######################################################################################################
    # loading photos
    time_print("starting photo load")
    if not(is_closed) and ad_data['photos'] is not None:
        photos_urls = [x['fullUrl'] for x in ad_data['photos']]
        photos_urls.extend(search_photos_url)
        shuffle(photos_urls)
        
    else:
        photos_urls = search_photos_url
    
    def sanitize_filename(url):
        return re.sub(r'[<>:"/\\|?*]', '_', url)

    photos_urls = set(photos_urls)
    if len(photos_urls) > 0:
        break_num = choice([10, 11, 12, 13, 14, 15])
        for index, photo_url in enumerate(photos_urls):
            save_file('image',  f"{path}/photos", sanitize_filename(photo_url), scraper.get(photo_url))

            await random_sleep(0.5, 0.5, prefix = scraper.proxy.split("@", 1)[1])

            # some extra sleep if too many photos
            if index % break_num == 0:
                await random_sleep(10, 0.5, prefix = scraper.proxy.split("@", 1)[1])

    shutil.move(path, dest_dir)

    return single_ad_df
